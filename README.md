## ğŸš€ Le stack ELK au service du data engineering ğŸ’» dans le secteur commercial des ordinateurs portables

### Plan
- [Introduction](#introduction)
- [PrÃ©requis](#prÃ©requis)
- [FonctionnalitÃ©](#fonctionnalitÃ©)
- [Installation](#Installation)
- [Utilisation](#utilisation)
- [Conclusion](#conclusion)

### âœ¨ Introduction
Dans le cadre de notre projet de data engineering appliquÃ© au secteur commercial des ordinateurs portables ğŸ’»ğŸ›ï¸, nous avons mis en place *un stack ELK* (Elasticsearch, Logstash, Kibana) ğŸ”ğŸ“Š. Cette architecture permet de structurer lâ€™ensemble du pipeline de traitement des donnÃ©es, depuis *leur gÃ©nÃ©ration ou collecte* jusquâ€™Ã  leur visualisation interactive sur Kibana.

Les donnÃ©es sont gÃ©nÃ©rÃ©es Ã  partir du *langage Python ğŸ* puis stockÃ©es dans *un fichier CSV ğŸ“„*. Ce fichier est ensuite *ingÃ©rÃ© et transformÃ© via Logstash*, avant dâ€™Ãªtre envoyÃ© vers *Elasticsearch* pour lâ€™indexation et lâ€™analyse. Enfin, les rÃ©sultats sont visualisÃ©s sur *Kibana*, offrant une interface claire et puissante pour explorer *les insights commerciaux*.

### ğŸ› ï¸ PrÃ©requis techniques
Avant de procÃ©der Ã  la mise en Å“uvre du stack ELK dans un contexte de data engineering, il est essentiel de disposer des compÃ©tences et outils suivants :

- *ğŸ–¥ï¸ MaÃ®trise du terminal* : Une familiaritÃ© avec lâ€™utilisation du terminal Linux (bash) ou du CMD sous Windows est requise pour exÃ©cuter les commandes et scripts nÃ©cessaires Ã  lâ€™orchestration des services.
- ğŸ Environnement Python : Il est recommandÃ© dâ€™avoir des connaissances de base en Python, ou Ã  minima de disposer dâ€™une installation fonctionnelle de Python sur votre machine ou dans votre environnement de travail.
- *ğŸ³ Infrastructure Docker* : La prÃ©sence de `Docker`, ainsi que de `Docker Compose` (`docker-compose` ou `docker compose` selon la version), est indispensable pour le dÃ©ploiement et la gestion des services `Elasticsearch`, `Logstash` et `Kibana` dans des conteneurs isolÃ©s.
- *ğŸ’» Shell Bash* : Assurez-vous que `bash` est installÃ© sur votre systÃ¨me, notamment pour lâ€™exÃ©cution de scripts dâ€™automatisation et de maintenance.
- *ğŸ“¦ Services ELK via Docker* : Les composants du *stack ELK â€” Elasticsearch*, *Kibana* et *Logstash* â€” doivent Ãªtre disponibles sur votre machine, idÃ©alement dÃ©ployÃ©s via `Docker` pour garantir `portabilitÃ©`, `reproductibilitÃ©` et `isolation des environnements`.


### âš™ï¸ FonctionnalitÃ©s principales du projet
- ğŸ“„ *GÃ©nÃ©ration des donnÃ©es* : CrÃ©ation *dâ€™un fichier CSV* contenant les donnÃ©es simulÃ©es ou rÃ©elles, placÃ© dans le rÃ©pertoire /data du projet pour Ãªtre utilisÃ© *dans le pipeline*.
- ğŸ³ *Configuration de lâ€™environnement ELK* : Mise en place des Ã©lÃ©ments de configuration via `Docker` et `Docker Compose`, incluant la crÃ©ation des volumes, des rÃ©seaux, et des services nÃ©cessaires au bon fonctionnement du *stack ELK*.
- ğŸ”„ *Ingestion des donnÃ©es* :
    Importation du fichier *CSV dans Elasticsearch* Ã  lâ€™aide de *Logstash*, selon les configurations dÃ©finies dans le pipeline.
    PossibilitÃ© dâ€™envoyer des donnÃ©es au *format JSON* directement vers *Elasticsearch*, soit via le script *setup-bi.sh*, soit via *Logstash* selon le scÃ©nario choisi.
- ğŸ“Š *Visualisation sur Kibana* : Exploration et analyse des *donnÃ©es ingÃ©rÃ©es* Ã  travers des tableaux de bord interactifs sur *Kibana*, permettant une lecture claire des tendances et indicateurs commerciaux.

### ğŸš€ Installation du projet

## ğŸ“¥ Cloner le dÃ©pÃ´t Git
 `git clone https://github.com/NABEDE/elk-docker-analyse-business.git`

## ğŸ“‚ Se placer dans le dossier du projet
`cd elk-docker-analyse-business`

## ğŸ³ Lancer lâ€™environnement Docker :
- Lancer le `docker-compose.yml` avec `docker-compose up -d` afin de crÃ©er l'environnement de travail et les volumes rattachÃ©s.

## ğŸ” VÃ©rifier le bon dÃ©marrage dâ€™Elasticsearch
- VÃ©rifier que Elasticsearch est bien lancÃ© en tapant cette commande : `curl -X GET http://localhost:9200/`, s'il n'y a aucune erreur qui apparaÃ®t, alors votre Elasticsearch est bien lancÃ©.

## ğŸ“„ VÃ©rifier le bon fonctionnement de Logstash
- VÃ©rifier aussi que votre Logstash est bien lancer en saisissant cette commande : `docker logs logstash`, s'il n'y a aucune erreur qui apparaÃ®t, alors votre Logstash est bien lancÃ©.

## ğŸ“¦ Importer les donnÃ©es dans Elasticsearch
- Lancer le script `setup-bi.sh` avec `bash setup-bi.sh` pour importer les donnÃ©es dans Elasticsearch.

## ğŸ“Š VÃ©rifier le bon fonctionnement de Kibana
- AccÃ©der Ã  Kibana en tapant cette commande : `http://localhost:5601/`, s'il n'y a aucune erreur qui apparaÃ®t, alors votre Kibana est bien lancÃ©.

## ğŸ¯ Lancer la boule d'exÃ©cution pour le dÃ©maraage du stack
- Cas exceptionnel, s'il y'a des coupures de session de votre environnement ELK utiliser cette commande avec ce setup, c'est comme une boule que vous lancez : `bash boule.sh`. Vous verez en temps rÃ©el le fonctionnement de vos containers et leur lancement, leur coupure ainsi de suite.

### ğŸ“ Conclusion

L'environnement **ELK** ğŸ§  est conÃ§u pour l'ingÃ©nierie des donnÃ©es ğŸ“Š. Dans le cadre de ce projet spÃ©cifique, il permet d'automatiser l'envoi ğŸš€, la crÃ©ation ğŸ› ï¸ ou la collecte des donnÃ©es ğŸ“¥, puis leur filtrage ğŸ§¹ avant de les intÃ©grer dans **Elasticsearch** ğŸ”.

L'utilisation de `setup` âš™ï¸ ne remplace ni ne supprime celle de `docker-compose` ou `docker compose` ğŸ³. Elle vise simplement Ã  optimiser l'automatisation du transit ğŸšš et du traitement des donnÃ©es ğŸ”„.

